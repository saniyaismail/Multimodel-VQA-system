{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing and Importing necessary libraries"
      ],
      "metadata": {
        "id": "572PvKFGCLJF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih__PpSzsaMk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate git+https://github.com/salesforce/BLIP.git -q\n",
        "!pip install torch torchvision -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "from PIL import Image\n",
        "import torch"
      ],
      "metadata": {
        "id": "3wZzRcBb7KG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the BLIP VQA model"
      ],
      "metadata": {
        "id": "YHOSfaNnCFOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load processor and model (BLIP base VQA)\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "processor.tokenizer = processor.tokenizer.__class__.from_pretrained(\n",
        "    processor.tokenizer.name_or_path, use_fast=True\n",
        ")\n",
        "\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(device)"
      ],
      "metadata": {
        "id": "8BE3J8nC96IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üîç Test with one image\n",
        "image_path = \"/content/abo-images-small/abo-images-small/00/00000529.jpg\"\n",
        "question = \"How many wheels does the object have? Answer in a ONE WORD ONLY\"\n",
        "\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "out = model.generate(**inputs)\n",
        "answer = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Q: {question}\\nA: {answer}\")"
      ],
      "metadata": {
        "id": "aR6ADZZi7H0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Data"
      ],
      "metadata": {
        "id": "lrqVq78nCAfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def extract_zip(zip_file_path, extract_to_path='.'):\n",
        "  try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_to_path)\n",
        "    print(f\"Successfully extracted '{zip_file_path}' to '{extract_to_path}'\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at '{zip_file_path}'\")\n",
        "  except zipfile.BadZipFile:\n",
        "    print(f\"Error: '{zip_file_path}' is not a valid zip file.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during extraction: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "zip_file = '/content/abo-images-small.zip'  # Replace with the actual path to your zip file\n",
        "extraction_directory = '/content/abo-images-small' # Replace with your desired extraction directory\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_directory, exist_ok=True)\n",
        "\n",
        "extract_zip(zip_file, extraction_directory)"
      ],
      "metadata": {
        "id": "Qi2Sxw-bGf35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv(\"/content/proper_result_2_final.csv\")\n",
        "\n",
        "# Add constraint to each question\n",
        "df['question'] = df['question'].apply(lambda q: f\"Answer in one word: {q.strip()}\")\n",
        "\n",
        "# Save back or to a new CSV\n",
        "df.to_csv(\"test_1_oneword.csv\", index=False)\n",
        "print(\"Updated CSV saved as test_1_oneword.csv with 'Answer in one word' prompt.\")\n"
      ],
      "metadata": {
        "id": "9HqZ8i-3Mqux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "MnbLrNv7zbFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Running BLIP Model on Each Image-Question Pair and generating results"
      ],
      "metadata": {
        "id": "I9JFQel8CXBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"test_1_oneword.csv\")\n",
        "\n",
        "# üëá Set your row range here\n",
        "start_idx = 25002\n",
        "end_idx = 45002  # exclusive, so rows 0 to 19\n",
        "subset_df = df.iloc[start_idx:end_idx]\n",
        "results = []\n",
        "\n",
        "for idx, row in tqdm(subset_df.iterrows(), total=len(subset_df), desc=\"Running BLIP on selected rows\"):\n",
        "    image_path = os.path.join(\"/content/abo-images-small/abo-images-small\", row['path'])  # change if needed\n",
        "    if not os.path.exists(image_path):\n",
        "        results.append(\"image_not_found\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        question = row[\"question\"]\n",
        "        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n",
        "        out = model.generate(**inputs)\n",
        "        decoded = processor.decode(out[0], skip_special_tokens=True)\n",
        "        answer = re.sub(r\"[^\\w]\", \"\", decoded.strip().split()[0])  # removes punctuation like '.' or ',' etc.\n",
        "    except Exception as e:\n",
        "        answer = f\"error: {str(e)}\"\n",
        "\n",
        "    results.append(answer)\n",
        "\n",
        "# Save result\n",
        "subset_df['blip_answer'] = results\n",
        "subset_df.to_csv(f\"blip_vqa_results_{start_idx}_{end_idx}.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "BQZ_r2rTHOqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the CSV\n",
        "df = pd.read_csv(\"/content/blip_vqa_results_15002_25002 (1).csv\")  # change to your actual file if different\n",
        "\n",
        "# Clean the blip_answer column\n",
        "def clean_answer(ans):\n",
        "    if pd.isna(ans):\n",
        "        return \"\"\n",
        "    ans = ans.strip()\n",
        "    first_word = ans.split()[0]\n",
        "    return re.sub(r\"[^\\w]\", \"\", first_word)  # remove punctuation\n",
        "\n",
        "df[\"blip_answer\"] = df[\"blip_answer\"].apply(clean_answer)\n",
        "\n",
        "# Save cleaned results\n",
        "df.to_csv(\"blip_vqa_results_2_cleaned.csv\", index=False)\n",
        "print(\"‚úÖ Cleaned file saved as blip_vqa_results_2_cleaned.csv\")\n"
      ],
      "metadata": {
        "id": "6glBz3wdWSch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "nERj2c1nBqW7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}