{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11814831,
          "sourceType": "datasetVersion",
          "datasetId": 7420874
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "0ftaMu89Pllx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/abo-images-small.zip -d /content/preprocessed_img\n"
      ],
      "metadata": {
        "id": "BclFP_nVe75c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering, default_data_collator"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:03:08.970386Z",
          "iopub.execute_input": "2025-05-14T22:03:08.971124Z",
          "iopub.status.idle": "2025-05-14T22:03:17.348597Z",
          "shell.execute_reply.started": "2025-05-14T22:03:08.971088Z",
          "shell.execute_reply": "2025-05-14T22:03:17.348002Z"
        },
        "id": "TAEL3MRIPISt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:11:18.632763Z",
          "iopub.execute_input": "2025-05-14T22:11:18.633615Z",
          "iopub.status.idle": "2025-05-14T22:11:18.637014Z",
          "shell.execute_reply.started": "2025-05-14T22:11:18.633587Z",
          "shell.execute_reply": "2025-05-14T22:11:18.636347Z"
        },
        "id": "XVjPlfWsPISv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv('/content/inference_final.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file 'inference_final.csv' was not found. Make sure it is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "# Define the train and test sizes based on the 80:20 ratio\n",
        "train_size = 0.9\n",
        "test_size = 0.1\n",
        "\n",
        "# Split the DataFrame into training and testing sets\n",
        "train_df, test_df = train_test_split(df, train_size=train_size, test_size=test_size, random_state=42)\n",
        "\n",
        "# Save the training DataFrame to train_1_new.csv\n",
        "train_df.to_csv('train_1_new.csv', index=False)\n",
        "print(f\"Training data saved to 'train_1_new.csv' with {len(train_df)} rows.\")\n",
        "\n",
        "# Save the testing DataFrame to test_1_new.csv\n",
        "test_df.to_csv('test_1_new.csv', index=False)\n",
        "print(f\"Testing data saved to 'test_1_new.csv' with {len(test_df)} rows.\")"
      ],
      "metadata": {
        "id": "hTvuWtNLU_Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Memory optimization\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Paths and parameters\n",
        "IMAGE_ROOT = \"/content/preprocessed_img/abo-images-small\"\n",
        "TRAIN_CSV = \"/content/train_1_new.csv\"\n",
        "TEST_CSV = \"/content/test_1_new.csv\"\n",
        "MODEL_SAVE_PATH = \"/content/Model/blip-saved-model-final\"\n",
        "BATCH_SIZE = 8  # Increased with gradient accumulation\n",
        "ACCUMULATION_STEPS = 2  # Effective batch size = 32\n",
        "NUM_EPOCHS = 4\n",
        "PATIENCE = 0  # More aggressive early stopping\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "# print(\"jbp\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:16:55.892123Z",
          "iopub.execute_input": "2025-05-14T22:16:55.892869Z",
          "iopub.status.idle": "2025-05-14T22:16:56.125762Z",
          "shell.execute_reply.started": "2025-05-14T22:16:55.892840Z",
          "shell.execute_reply": "2025-05-14T22:16:56.125121Z"
        },
        "id": "Yp-2iaHUPISv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, dataframe, image_root, processor, max_question_len=32, max_answer_len=8):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_root = image_root\n",
        "        self.processor = processor\n",
        "        self.max_question_len = max_question_len\n",
        "        self.max_answer_len = max_answer_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def _load_image(self, row):\n",
        "        base_path = os.path.join(self.image_root, os.path.splitext(row['path'])[0])\n",
        "        for ext in ('.jpeg', '.jpg'):\n",
        "            full = base_path + ext\n",
        "            if os.path.exists(full):\n",
        "                return Image.open(full).convert(\"RGB\")\n",
        "        raise FileNotFoundError(f\"Image not found for {row['path']} at {base_path}.[jpeg/jpg]\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image = self._load_image(row)\n",
        "\n",
        "        prompt = \"Answer in a single word: \" + row['question']\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_question_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels = self.processor.tokenizer(\n",
        "            str(row['answer']),\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_answer_len,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        return {\n",
        "            'pixel_values':   inputs.pixel_values.squeeze(0),\n",
        "            'input_ids':      inputs.input_ids.squeeze(0),\n",
        "            'attention_mask': inputs.attention_mask.squeeze(0),\n",
        "            'labels':         labels.squeeze(0),\n",
        "        }\n",
        "\n",
        "\n",
        "class VQATestDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_root, processor, max_question_len=32):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_root = image_root\n",
        "        self.processor = processor\n",
        "        self.max_question_len = max_question_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def _load_image(self, row):\n",
        "        base_path = os.path.join(self.image_root, os.path.splitext(row['path'])[0])\n",
        "        for ext in ('.jpeg', '.jpg'):\n",
        "            full = base_path + ext\n",
        "            if os.path.exists(full):\n",
        "                return Image.open(full).convert(\"RGB\")\n",
        "        raise FileNotFoundError(f\"Image not found for {row['path']} at {base_path}.[jpeg/jpg]\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image = self._load_image(row)\n",
        "\n",
        "        prompt = \"Answer in a single word: \" + row['question']\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=prompt,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_question_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'pixel_values':   inputs.pixel_values.squeeze(0),\n",
        "            'input_ids':      inputs.input_ids.squeeze(0),\n",
        "            'attention_mask': inputs.attention_mask.squeeze(0),\n",
        "        }\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:16:56.127207Z",
          "iopub.execute_input": "2025-05-14T22:16:56.127478Z",
          "iopub.status.idle": "2025-05-14T22:16:56.136121Z",
          "shell.execute_reply.started": "2025-05-14T22:16:56.127460Z",
          "shell.execute_reply": "2025-05-14T22:16:56.135493Z"
        },
        "id": "SUUUgr1XPISv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "Jq8QjHmdWXtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define your file paths (assuming these are defined elsewhere)\n",
        "# TRAIN_CSV = 'train.csv'  # Replace with your actual train CSV path\n",
        "# TEST_CSV = 'test.csv'    # Replace with your actual test CSV path\n",
        "\n",
        "# Load data\n",
        "full_train_df = pd.read_csv(\"/content/train_1_new.csv\")\n",
        "test_df       = pd.read_csv(\"/content/test_1_new.csv\")\n",
        "\n",
        "# Step 1: Sample 10% from train.csv (≈8.5% train, 1.5% val)\n",
        "train_val_df = full_train_df.sample(frac=1, random_state=42)\n",
        "train_df, val_from_train_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.15,  # 15% of that 10% → 1.5% of full_train_df\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Step 2: Sample 5% from test.csv for validation\n",
        "val_from_test_df = test_df.sample(frac=0.05, random_state=42)\n",
        "\n",
        "# Step 3: Build final splits\n",
        "val_df = pd.concat([val_from_train_df, val_from_test_df]).reset_index(drop=True)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Sanity check - Print the sizes of the final DataFrames\n",
        "print(f\"Final Dataframe sizes: Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "KDWT723NVyNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and processor\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\n",
        "    \"Salesforce/blip-vqa-base\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing BEFORE wrapping with DataParallel\n",
        "# model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=10,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "# Prepare model for LoRA fine-tuning (QLoRA-compatible setup)\n",
        "# model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters to verify LoRA\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Move model to device FIRST\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Wrap with DataParallel ONLY ONCE if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for DataParallel\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    VQADataset(train_df, IMAGE_ROOT, processor),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_data_collator,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    VQADataset(val_df, IMAGE_ROOT, processor),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    collate_fn=default_data_collator,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    VQATestDataset(test_df, IMAGE_ROOT, processor),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:16:56.136793Z",
          "iopub.execute_input": "2025-05-14T22:16:56.136996Z",
          "iopub.status.idle": "2025-05-14T22:16:58.335437Z",
          "shell.execute_reply.started": "2025-05-14T22:16:56.136971Z",
          "shell.execute_reply": "2025-05-14T22:16:58.334550Z"
        },
        "id": "53TKmmWgPISv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer and GradScaler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Training setup\n",
        "tracking_info = []\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "# Training loop with gradient accumulation\n",
        "NUM_EPOCHS = 5\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    train_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss.mean() / ACCUMULATION_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % ACCUMULATION_STEPS == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**{k: v.to(DEVICE) for k, v in batch.items()})\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    epoch_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} — \" f\"Train Loss: {epoch_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "    tracking_info.append((train_loss/len(train_loader), avg_val_loss))\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        model.save_pretrained(MODEL_SAVE_PATH)\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= 0:\n",
        "            break\n",
        "\n",
        "# Save results\n",
        "with open(\"training_logs.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tracking_info, f)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-14T22:17:28.644015Z",
          "iopub.execute_input": "2025-05-14T22:17:28.644386Z",
          "iopub.status.idle": "2025-05-14T22:17:30.246755Z",
          "shell.execute_reply.started": "2025-05-14T22:17:28.644356Z",
          "shell.execute_reply": "2025-05-14T22:17:30.244925Z"
        },
        "id": "jRjiBHBpPISv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(model, processor, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model.module.generate(**batch) if hasattr(model, \"module\") else model.generate(**batch)\n",
        "        preds = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "        predictions.extend(preds)\n",
        "    return predictions\n",
        "\n",
        "# Load model and generate predictions\n",
        "# Specify `local_files_only=True` to load from the local path\n",
        "model = BlipForQuestionAnswering.from_pretrained(MODEL_SAVE_PATH, local_files_only=True).to(DEVICE)\n",
        "test_preds = generate_predictions(model, processor, test_loader, DEVICE)\n",
        "\n",
        "# Save to CSV\n",
        "test_df[\"predicted_answer\"] = test_preds\n",
        "test_df.to_csv(\"test_predictions_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "tSmR5OyRRPDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Load the training logs\n",
        "with open(\"training_logs.pkl\", \"rb\") as f:\n",
        "    tracking_info = pickle.load(f)\n",
        "\n",
        "# Extract training and validation losses\n",
        "train_losses = [item[0] for item in tracking_info]\n",
        "val_losses = [item[1] for item in tracking_info]\n",
        "epochs = range(1, len(tracking_info) + 1)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss', marker='x')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(epochs)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irY7vIvjg0QR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd\n",
        "\n",
        "# Load single result CSV\n",
        "df = pd.read_csv(\"/content/test_predictions_final.csv\")\n",
        "\n",
        "# Ensure lowercase, string type\n",
        "df['answer'] = df['answer'].astype(str).str.lower()\n",
        "df['predicted_answer'] = df['predicted_answer'].astype(str).str.lower()\n",
        "\n",
        "# Extract lists for comparison\n",
        "predictions = df['predicted_answer'].tolist()\n",
        "refs = df['answer'].tolist()\n",
        "\n",
        "# Compute exact-match binary metrics\n",
        "y_pred_bin = [int(p == r) for p, r in zip(predictions, refs)]\n",
        "y_true_bin = [1] * len(refs)\n",
        "\n",
        "acc = accuracy_score(y_true_bin, y_pred_bin)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "    y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n",
        ")\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Exact-match Accuracy: {acc:.3f}\")\n",
        "print(f\"Exact-match Precision: {prec:.3f}\")\n",
        "print(f\"Exact-match Recall:    {rec:.3f}\")\n",
        "print(f\"Exact-match F1:        {f1:.3f}\")\n"
      ],
      "metadata": {
        "id": "kIaboqAzmhwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert score"
      ],
      "metadata": {
        "id": "ZRWKtm9Sg88K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from bert_score import score as bert_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load single result CSV\n",
        "df = pd.read_csv(\"test_predictions_final.csv\")\n",
        "\n",
        "# Ensure lowercase, string type\n",
        "df['answer'] = df['answer'].astype(str).str.lower()\n",
        "df['predicted_answer'] = df['predicted_answer'].astype(str).str.lower()\n",
        "\n",
        "# Extract lists for comparison\n",
        "predictions = df['predicted_answer'].tolist()\n",
        "refs = df['answer'].tolist()\n",
        "\n",
        "# Compute exact-match binary metrics\n",
        "y_pred_bin = [int(p == r) for p, r in zip(predictions, refs)]\n",
        "y_true_bin = [1] * len(refs)\n",
        "\n",
        "acc = accuracy_score(y_true_bin, y_pred_bin)\n",
        "prec, rec, f1 = precision_recall_fscore_support(\n",
        "    y_true_bin, y_pred_bin, average=\"binary\", zero_division=0\n",
        ")[:3]\n",
        "\n",
        "# Compute BERTScore F1\n",
        "P, R, F1 = bert_score(predictions, refs, lang=\"en\", rescale_with_baseline=True)\n",
        "bert_f1 = F1.mean().item()\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Exact-match Accuracy: {acc:.3f}\")\n",
        "print(f\"Exact-match Precision: {prec:.3f}\")\n",
        "print(f\"Exact-match Recall:    {rec:.3f}\")\n",
        "print(f\"Exact-match F1:        {f1:.3f}\")\n",
        "print(f\"BERTScore F1:          {bert_f1:.3f}\")"
      ],
      "metadata": {
        "id": "y1Nr7pryTdk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}